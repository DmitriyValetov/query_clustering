# Утилита векторизации поисковых запросов

Утилита для векторизации поисковых запросов с сохранением каждого вектора в отдельный файл.

## Оглавление

Репозиторий содержит следующие утилиты для обработки и кластеризации поисковых запросов:

1. **`cli_vectorize_queries.py`** - Векторизация поисковых запросов через GigaChat Embeddings
2. **`cli_reduce_dimensions_umap.py`** - Снижение размерности векторов методом UMAP
3. **`cli_cluster_dbscan.py`** - Кластеризация векторов методом DBSCAN
4. **`cli_cluster_themes.py`** - Распознавание тем в кластерах через классификацию запросов

## Требования

- Python 3.8+
- Виртуальное окружение (рекомендуется)

## Установка

1. Создайте виртуальное окружение:
```bash
python -m venv env
source env/bin/activate  # Linux/Mac
# или
env\Scripts\activate  # Windows
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

3. Создайте файл `.env` с настройками:
```env
GIGACHAT_BASE_URL=https://your-gigachat-url
GIGACHAT_KEY=your-api-key
```

## Использование

### `cli_vectorize_queries.py` - Векторизация запросов

Векторизует поисковые запросы и сохраняет каждый вектор в отдельный файл с именем `{query_id}.npz`.

**Параметры:**
- `-i, --input` - Входные CSV файлы (можно указать несколько, по умолчанию: `data.csv`)
- `-o, --output` - Папка для сохранения векторов (по умолчанию: `cache_vectors`)
- `-w, --workers` - Количество параллельных потоков для векторизации (по умолчанию: 5)
- `-c, --chunk-size` - Размер чанка для чтения из CSV (по умолчанию: 10000)
- `-l, --limit` - Ограничение количества новых запросов для обработки (по умолчанию: без ограничений)

**Примеры использования:**
```bash
# Использование файлов по умолчанию
python cli_vectorize_queries.py -o cache_vectors

# Указание конкретных файлов
python cli_vectorize_queries.py -i data.csv -o cache_vectors

# С ограничением количества и настройкой параллельных потоков
python cli_vectorize_queries.py -i data.csv -o vectors -l 1000 -w 10

# Полная настройка всех параметров
python cli_vectorize_queries.py -i file1.csv file2.csv -o output_dir -w 8 -c 5000 -l 500
```

**Выходные данные:**
- Векторы сохраняются в указанную папку как `{query_id}.npz`
- Метаданные сохраняются в `{output_dir}/metadata.json`

### `cli_reduce_dimensions_umap.py` - Снижение размерности векторов методом UMAP

Снижает размерность векторов с помощью алгоритма UMAP, сохраняя каждый сжатый вектор в отдельный файл.

**Параметры:**
- `-i, --input-dir` - Папка с исходными векторами (индивидуальные .npz файлы)
- `-o, --output-dir` - Папка для сохранения сжатых векторов
- `-d, --dimensions` - Размерность итоговых векторов

**Примеры использования:**
```bash
# Снижение размерности до 128
python cli_reduce_dimensions_umap.py -i cache_vectors_unpacked -o cache_vectors_reduced -d 128

# Снижение размерности до 64
python cli_reduce_dimensions_umap.py -i cache_vectors_unpacked -o cache_vectors_reduced -d 64
```

**Примечания:**
- Утилита загружает все векторы в память для обучения UMAP модели
- После обучения применяет трансформацию ко всем векторам и сохраняет результаты
- Сохраняет `query_id` и `query_text` вместе с каждым сжатым вектором

### `cli_cluster_dbscan.py` - Кластеризация векторов методом DBSCAN

Выполняет кластеризацию векторов методом DBSCAN и сохраняет результаты в CSV файл.

**Параметры:**
- `-i, --input-dir` - Папка с векторами (индивидуальные .npz файлы)
- `-o, --output` - Путь к выходному CSV файлу с результатами кластеризации
- `--eps` - Максимальное расстояние между точками в одном кластере (опционально, определяется автоматически)
- `--min-samples` - Минимальное количество точек для формирования кластера (по умолчанию: log(n) или 5)
- `--auto-eps` - Автоматически определить eps через k-distance graph
- `--k-neighbors` - Количество соседей для k-distance graph (по умолчанию: 5)
- `--metric` - Метрика расстояния: `cosine` или `euclidean` (по умолчанию: cosine)
- `--stats-output` - Путь к JSON файлу со статистикой (по умолчанию: рядом с CSV файлом)

**Примеры использования:**
```bash
# Автоматическое определение eps
python cli_cluster_dbscan.py -i cache_vectors_reduced -o clusters.csv --auto-eps

# Ручное указание параметров
python cli_cluster_dbscan.py -i cache_vectors_reduced -o clusters.csv --eps 0.5 --min-samples 10

# С указанием метрики и статистики
python cli_cluster_dbscan.py -i cache_vectors_reduced -o clusters.csv --eps 0.5 --min-samples 5 --metric euclidean --stats stats.json
```

**Выходные данные:**
- CSV файл с колонками: `id`, `cluster` (где -1 означает шум/outlier)
- JSON файл со статистикой: количество кластеров, размеры кластеров, процент шума, параметры

**Примечания:**
- Утилита автоматически определяет оптимальное значение `eps` через k-distance graph, если не указано вручную
- Для `min_samples` используется эвристика log(n) или минимум 5
- Для больших датасетов используется выборка для оценки eps

### `cli_cluster_themes.py` - Распознавание основных тем в кластерах

Анализирует запросы в каждом кластере через классификацию в GigaChat и определяет теги/категории для кластеров.

**Параметры:**
- `-c, --clusters` - Путь к CSV файлу с кластерами (колонки: id, cluster)
- `-o, --output-dir` - Папка для сохранения JSON файлов с темами кластеров
- `--csv-files` - Пути к CSV файлам с запросами (можно указать несколько файлов, обязательный)
- `--max-queries` - Максимум запросов для анализа на кластер (по умолчанию: 100)

**Примеры использования:**
```bash
# Базовое использование
python cli_cluster_themes.py -c clusters.csv -o cluster_themes --csv-files data.csv

# С ограничением количества запросов на кластер
python cli_cluster_themes.py -c clusters.csv -o cluster_themes --csv-files data.csv web_search.csv --max-queries 50
```

**Выходные данные:**
- Для каждого кластера создается JSON файл `cluster_{id}.json` со структурой:
  - `unique_tags` - список уникальных категорий из обработанных запросов
  - `general_tags` - общее описание тегов/тем кластера (генерируется через LLM)
  - `individual_tags` - словарь {query_id: category} для каждого запроса

**Примечания:**
- Обрабатывает до 100 запросов на кластер (настраивается через `--max-queries`)
- Отправляет запросы в GigaChat батчами по 5 запросов, 5 батчей параллельно
- Использует промпты из `promts.py` для классификации
- Автоматически генерирует общее описание тегов кластера на основе категорий

## Структура данных

### Входные CSV файлы

#### Формат `data.csv`

Файл `data.csv` используется как входной файл по умолчанию для утилит `cli_vectorize_queries.py` и `cli_cluster_themes.py`.

**Обязательные колонки:**
- `text` (string) - Текст поискового запроса пользователя. Обязательное поле, не должно быть пустым.
- `gigasearch_trigger` (integer) - Флаг триггера для обработки запроса. Значение `1` означает, что запрос должен быть обработан, другие значения игнорируются.

**Пример содержимого `data.csv`:**
```csv
text,gigasearch_trigger
"найти информацию о Python",1
"как создать презентацию",1
"сравнить цены на ноутбуки",1
"объяснить ошибку в коде",1
"не обрабатывать этот запрос",0
```

**Примечания:**
- Файл может содержать дополнительные колонки, они будут проигнорированы
- Обрабатываются только строки с `gigasearch_trigger == 1`
- Строки с пустым или отсутствующим полем `text` пропускаются
- Поддерживается обработка нескольких CSV файлов одновременно

### Формат векторов

Каждый вектор сохраняется в файле `{query_id}.npz` со следующими ключами:
- `query_id` - ID запроса (генерируется из текста запроса)
- `query_text` - Текст запроса
- `vector` - Массив чисел (вектор)

### Метаданные

#### `cli_vectorize_queries.py` - Метаданные векторизации

Файл `{output_dir}/metadata.json` содержит информацию о всех векторизованных запросах:

```json
{
  "total_vectors": 1000,
  "query_ids": ["id1", "id2", "id3", ...]
}
```

**Поля:**
- `total_vectors` (integer) - Общее количество сохраненных векторов
- `query_ids` (array of strings) - Список всех ID запросов, для которых созданы векторы

**Примечания:**
- Метаданные сохраняются периодически (каждые 100 векторов) и финально в конце обработки
- Используются для возобновления обработки - уже обработанные запросы пропускаются

#### `cli_cluster_dbscan.py` - Результаты кластеризации

**CSV файл** (`{output}.csv`) содержит результаты кластеризации:

```csv
id,cluster
abc123def456,0
def456ghi789,0
ghi789jkl012,1
jkl012mno345,-1
```

**Поля:**
- `id` (string) - ID запроса (query_id)
- `cluster` (integer) - Номер кластера. Значение `-1` означает шум/outlier (точка не попала ни в один кластер)

**JSON файл со статистикой** (`{output}_stats.json` или указанный через `--stats-output`):

```json
{
  "total_points": 1000,
  "n_clusters": 15,
  "n_noise": 50,
  "noise_percentage": 5.0,
  "parameters": {
    "eps": 0.5,
    "min_samples": 10
  },
  "cluster_sizes": {
    "0": 150,
    "1": 120,
    "2": 100,
    ...
  }
}
```

**Поля:**
- `total_points` (integer) - Общее количество обработанных точек
- `n_clusters` (integer) - Количество найденных кластеров
- `n_noise` (integer) - Количество шумовых точек (outliers)
- `noise_percentage` (float) - Процент шумовых точек от общего количества
- `parameters` (object) - Параметры кластеризации:
  - `eps` (float) - Максимальное расстояние между точками в одном кластере
  - `min_samples` (integer) - Минимальное количество точек для формирования кластера
- `cluster_sizes` (object) - Словарь размеров кластеров: `{cluster_id: size}`, отсортированный по убыванию размера

#### `cli_cluster_themes.py` - Темы кластеров

Для каждого кластера создается JSON файл `cluster_{cluster_id}.json` в указанной выходной папке:

```json
{
  "unique_tags": [
    "Найти информацию",
    "Сравнить сущности",
    "Решить задачу"
  ],
  "general_tags": "Кластер содержит запросы, связанные с поиском информации и сравнением различных объектов. Пользователи стремятся найти данные и провести их анализ для решения конкретных задач.",
  "individual_tags": {
    "abc123def456": "Найти информацию",
    "def456ghi789": "Сравнить сущности",
    "ghi789jkl012": "Решить задачу"
  }
}
```

**Поля:**
- `unique_tags` (array of strings) - Список уникальных категорий, определенных для запросов в кластере
- `general_tags` (string) - Общее описание тегов/тем кластера, сгенерированное через LLM на основе категорий
- `individual_tags` (object) - Словарь соответствий `{query_id: category}` для каждого обработанного запроса в кластере

**Примечания:**
- Обрабатывается максимум 100 запросов на кластер (настраивается через `--max-queries`)
- Категории определяются через классификацию в GigaChat с использованием промптов из `promts.py`

## Примечания

- Утилита поддерживает возобновление обработки - уже обработанные запросы пропускаются
- Векторы сохраняются по одному в файл для удобства работы с большими объемами данных
- Метаданные сохраняются периодически (каждые 100 векторов) и финально в конце обработки

