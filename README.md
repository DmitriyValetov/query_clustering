# Утилита векторизации поисковых запросов

Утилита для векторизации поисковых запросов с сохранением каждого вектора в отдельный файл.

## Требования

- Python 3.8+
- Виртуальное окружение (рекомендуется)

## Установка

1. Создайте виртуальное окружение:
```bash
python -m venv env
source env/bin/activate  # Linux/Mac
# или
env\Scripts\activate  # Windows
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

3. Создайте файл `.env` с настройками:
```env
GIGACHAT_BASE_URL=https://your-gigachat-url
GIGACHAT_KEY=your-api-key
```

## Использование

### `cli_vectorize_queries.py` - Векторизация запросов

Векторизует поисковые запросы и сохраняет каждый вектор в отдельный файл с именем `{query_id}.npz`.

**Параметры:**
- `-i, --input` - Входные CSV файлы (можно указать несколько, по умолчанию: `data.csv`)
- `-o, --output` - Папка для сохранения векторов (по умолчанию: `cache_vectors`)
- `-w, --workers` - Количество параллельных потоков для векторизации (по умолчанию: 5)
- `-c, --chunk-size` - Размер чанка для чтения из CSV (по умолчанию: 10000)
- `-l, --limit` - Ограничение количества новых запросов для обработки (по умолчанию: без ограничений)

**Примеры использования:**
```bash
# Использование файлов по умолчанию
python cli_vectorize_queries.py -o cache_vectors

# Указание конкретных файлов
python cli_vectorize_queries.py -i data.csv -o cache_vectors

# С ограничением количества и настройкой параллельных потоков
python cli_vectorize_queries.py -i data.csv -o vectors -l 1000 -w 10

# Полная настройка всех параметров
python cli_vectorize_queries.py -i file1.csv file2.csv -o output_dir -w 8 -c 5000 -l 500
```

**Выходные данные:**
- Векторы сохраняются в указанную папку как `{query_id}.npz`
- Метаданные сохраняются в `{output_dir}/metadata.json`

### `cli_reduce_dimensions_umap.py` - Снижение размерности векторов методом UMAP

Снижает размерность векторов с помощью алгоритма UMAP, сохраняя каждый сжатый вектор в отдельный файл.

**Параметры:**
- `-i, --input-dir` - Папка с исходными векторами (индивидуальные .npz файлы)
- `-o, --output-dir` - Папка для сохранения сжатых векторов
- `-d, --dimensions` - Размерность итоговых векторов

**Примеры использования:**
```bash
# Снижение размерности до 128
python cli_reduce_dimensions_umap.py -i cache_vectors_unpacked -o cache_vectors_reduced -d 128

# Снижение размерности до 64
python cli_reduce_dimensions_umap.py -i cache_vectors_unpacked -o cache_vectors_reduced -d 64
```

**Примечания:**
- Утилита загружает все векторы в память для обучения UMAP модели
- После обучения применяет трансформацию ко всем векторам и сохраняет результаты
- Сохраняет `query_id` и `query_text` вместе с каждым сжатым вектором

### `cli_cluster_dbscan.py` - Кластеризация векторов методом DBSCAN

Выполняет кластеризацию векторов методом DBSCAN и сохраняет результаты в CSV файл.

**Параметры:**
- `-i, --input-dir` - Папка с векторами (индивидуальные .npz файлы)
- `-o, --output` - Путь к выходному CSV файлу с результатами кластеризации
- `--eps` - Максимальное расстояние между точками в одном кластере (опционально, определяется автоматически)
- `--min-samples` - Минимальное количество точек для формирования кластера (по умолчанию: log(n) или 5)
- `--auto-eps` - Автоматически определить eps через k-distance graph
- `--k-neighbors` - Количество соседей для k-distance graph (по умолчанию: 5)
- `--metric` - Метрика расстояния: `cosine` или `euclidean` (по умолчанию: cosine)
- `--stats-output` - Путь к JSON файлу со статистикой (по умолчанию: рядом с CSV файлом)

**Примеры использования:**
```bash
# Автоматическое определение eps
python cli_cluster_dbscan.py -i cache_vectors_reduced -o clusters.csv --auto-eps

# Ручное указание параметров
python cli_cluster_dbscan.py -i cache_vectors_reduced -o clusters.csv --eps 0.5 --min-samples 10

# С указанием метрики и статистики
python cli_cluster_dbscan.py -i cache_vectors_reduced -o clusters.csv --eps 0.5 --min-samples 5 --metric euclidean --stats stats.json
```

**Выходные данные:**
- CSV файл с колонками: `id`, `cluster` (где -1 означает шум/outlier)
- JSON файл со статистикой: количество кластеров, размеры кластеров, процент шума, параметры

**Примечания:**
- Утилита автоматически определяет оптимальное значение `eps` через k-distance graph, если не указано вручную
- Для `min_samples` используется эвристика log(n) или минимум 5
- Для больших датасетов используется выборка для оценки eps

### `cli_cluster_themes.py` - Распознавание основных тем в кластерах

Анализирует запросы в каждом кластере через классификацию в GigaChat и определяет теги/категории для кластеров.

**Параметры:**
- `-c, --clusters` - Путь к CSV файлу с кластерами (колонки: id, cluster)
- `-o, --output-dir` - Папка для сохранения JSON файлов с темами кластеров
- `--csv-files` - Пути к CSV файлам с запросами (можно указать несколько файлов, обязательный)
- `--max-queries` - Максимум запросов для анализа на кластер (по умолчанию: 100)

**Примеры использования:**
```bash
# Базовое использование
python cli_cluster_themes.py -c clusters.csv -o cluster_themes --csv-files data.csv

# С ограничением количества запросов на кластер
python cli_cluster_themes.py -c clusters.csv -o cluster_themes --csv-files data.csv web_search.csv --max-queries 50
```

**Выходные данные:**
- Для каждого кластера создается JSON файл `cluster_{id}.json` со структурой:
  - `unique_tags` - список уникальных категорий из обработанных запросов
  - `general_tags` - общее описание тегов/тем кластера (генерируется через LLM)
  - `individual_tags` - словарь {query_id: category} для каждого запроса

**Примечания:**
- Обрабатывает до 100 запросов на кластер (настраивается через `--max-queries`)
- Отправляет запросы в GigaChat батчами по 5 запросов, 5 батчей параллельно
- Использует промпты из `promts.py` для классификации
- Автоматически генерирует общее описание тегов кластера на основе категорий

## Структура данных

### Входные CSV файлы

Ожидаемые колонки:
- `text` - Текст поискового запроса
- `gigasearch_trigger` - Флаг триггера (1 для обработки)

### Формат векторов

Каждый вектор сохраняется в файле `{query_id}.npz` со следующими ключами:
- `query_id` - ID запроса (генерируется из текста запроса)
- `query_text` - Текст запроса
- `vector` - Массив чисел (вектор)

### Метаданные

Файл `metadata.json` содержит:
```json
{
  "total_vectors": 1000,
  "query_ids": ["id1", "id2", ...]
}
```

## Примечания

- Утилита поддерживает возобновление обработки - уже обработанные запросы пропускаются
- Векторы сохраняются по одному в файл для удобства работы с большими объемами данных
- Метаданные сохраняются периодически (каждые 100 векторов) и финально в конце обработки

